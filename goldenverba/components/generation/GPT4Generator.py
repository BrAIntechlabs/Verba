import os, json, openai
from dotenv import load_dotenv
from goldenverba.components.interfaces import Generator

load_dotenv()

class GPT4Generator(Generator):
    """
    GPT4 Generator.
    """

    def __init__(self):
        super().__init__()
        self.name = "GPT4Generator"
        self.description = "Generator using OpenAI's GPT-4-1106-preview model"
        self.requires_library = ["openai"]
        self.requires_env = ["OPENAI_API_KEY"]
        self.streamable = True
        self.model_name = os.getenv("OPENAI_MODEL", "gpt-4-1106-preview")
        self.context_window = 10000

    async def generate_stream(
        self,
        queries: list[str],
        context: list[str],
        conversation: dict = None,
    ):
        """Generate a stream of response dicts based on a list of queries and list of contexts, and includes conversational context
        @parameter: queries : list[str] - List of queries
        @parameter: context : list[str] - List of contexts
        @parameter: conversation : dict - Conversational context
        @returns Iterator[dict] - Token response generated by the Generator in this format {system:TOKEN, finish_reason:stop or empty}.
        """

        url = os.environ.get("OPENAI_API_KEY", "")
        if url == "":
            yield {
                "message": "Missing OpenAI API Key",
                "finish_reason": "stop",
            }

        if conversation is None:
            conversation = {}
        customprompt = self.run_function(conversation)
        messages = self.prepare_messages(queries, context, conversation, customprompt)

        try:

            openai.api_key = os.getenv("OPENAI_API_KEY")
            base_url = os.environ.get("OPENAI_BASE_URL", "")
            if base_url:
                openai.api_base = base_url

            if "OPENAI_API_TYPE" in os.environ:
                openai.api_type = os.getenv("OPENAI_API_TYPE")
            if "OPENAI_API_BASE" in os.environ:
                openai.api_base = os.getenv("OPENAI_API_BASE")
            if "OPENAI_API_VERSION" in os.environ:
                openai.api_version = os.getenv("OPENAI_API_VERSION")

            

            chat_completion_arguments = {
                "model": self.model_name,
                "messages": messages,
                "stream": True,
                "temperature": 0.0,
            }
            if openai.api_type == "azure":
                chat_completion_arguments["deployment_id"] = self.model_name

            completion = await openai.ChatCompletion.acreate(
                **chat_completion_arguments
            )

            try:
                while True:
                    chunk = await completion.__anext__()
                    if len(chunk["choices"]) > 0:
                        if "content" in chunk["choices"][0]["delta"]:
                            yield {
                                "message": chunk["choices"][0]["delta"]["content"],
                                "finish_reason": chunk["choices"][0]["finish_reason"],
                            }
                        else:
                            yield {
                                "message": "",
                                "finish_reason": chunk["choices"][0]["finish_reason"],
                            }
            except StopAsyncIteration:
                pass

        except Exception:
            raise

    def run_function(self,conversation):
        model = os.getenv("OPENAI_MODEL", "gpt-4-1106-preview")
        with open('Verba/goldenverba/components/generation/Functions.json') as f:
            functions = json.load(f)
        messages = []
        for message in conversation:
            if message.type != 'system':
                messages.append({"role": message.type, "content": message.content})

        response= openai.ChatCompletion.create(
                model=model,
                messages=messages,
                functions=functions,
                )
        print(response)
        response_message = response["choices"][0]["message"]
        #check to see if the response is a function call
            
        if response_message.get("function_call"):
            # got a response back for a function call
            print("FUNCTION MODE!")
            available_functions = {
                "identify_user": self.identify_user
            }  # only one function in this example, but you can have multiple
            function_name = response_message["function_call"]["name"]
            function_to_call = available_functions[function_name]
            function_args = json.loads(response_message["function_call"]["arguments"])
            if function_name == "identify_user":

                function_response = function_to_call(
                    user_expertise=function_args.get("user_expertise"),
                    language_preference=function_args.get("language_preference"),
                    brevity=function_args.get("brevity")
                )    
                       
        return(function_response)

    def identify_user(user_expertise, language_preference, brevity):
        """
        Function to identify the user based on the provided expertise, language preference, and brevity.
        @parameter user_expertise : str - User expertise
        @parameter language_preference : str - User language preference
        @parameter brevity : str - User brevity
        @returns str - Identified user
        """
        message = [ {"role": "system",
                "content": f"You are a chat-bot that specializes in RAG (retreival augemented generation).  You are connected to a vector database and have the ability to pull context into the prompts.  Always use this context to answer and questions the user has.  Do not make up facts and if you are unable to answer the question, simply tell the user that. The user is identified with expertise: {user_expertise}, language preference: {language_preference}, and brevity: {brevity}.  Use this information when responding back to the user and try to respond back in a way that is specific to the user. Always respond back in the language preference but if that is not know then respond in English."
                    }]
        
        return message

    def prepare_messages(
        self, queries: list[str], context: list[str], conversation: dict[str, str], customprompt: str
    ) -> dict[str, str]:
        """
        Prepares a list of messages formatted for a Retrieval Augmented Generation chatbot system, including system instructions, previous conversation, and a new user query with context.

        @parameter queries: A list of strings representing the user queries to be answered.
        @parameter context: A list of strings representing the context information provided for the queries.
        @parameter conversation: A list of previous conversation messages that include the role and content.

        @returns A list of message dictionaries formatted for the chatbot. This includes an initial system message, the previous conversation messages, and the new user query encapsulated with the provided context.

        Each message in the list is a dictionary with 'role' and 'content' keys, where 'role' is either 'system' or 'user', and 'content' contains the relevant text. This will depend on the LLM used.
        """
        #Note: The this is where the generative feedback loop should start to identify who the user is
        
        if customprompt == "":
            customprompt = "You are a chat-bot that specializes in RAG (retreival augemented generation).  You are connected to a vector database and have the ability to pull context into the prompts.  Always use this context to answer and questions the user has.  Do not make up facts and if you are unable to answer the question, simply tell the user that."

        messages = [
            {
                "role": "system",
                "content": customprompt,
            }
        ]
        print(messages)

        for message in conversation:
            if message.type != "system":
                messages.append({"role": message.type, "content": message.content})

        query = " ".join(queries)
        user_context = " ".join(context)

        messages.append(
            {
                "role": "user",
                "content": f"Please answer this query: '{query}' with this provided context: {user_context}",
            }
        )

        return messages
